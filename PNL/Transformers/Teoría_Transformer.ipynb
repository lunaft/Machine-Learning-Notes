{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura del transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificador\n",
    "\n",
    "El codificador se centra en entender la frase de entrada.\n",
    "\n",
    "2 pasos:\n",
    "\n",
    "- **Auto atención** (self-attention)\n",
    "- **Propagación hacia adelante** (feed forward)\n",
    "\n",
    "Ejem:\n",
    "\n",
    "'Soy de Málaga'\n",
    "\n",
    "Como no podemos pasar las palabras al modelo, le pasamos los embedings. Pasamos un embeding por cada palabra al mecanísmo de auto atención.\n",
    "\n",
    "Ese mecanismo de auto atención, nos devuelve vectores de contexto.\n",
    "\n",
    "Le pasamos los vectores de contexto al mecanismo de propagación hacia adelante y nos devuelve el restultado.\n",
    "\n",
    "Así funciona la auto-atencion: https://www.youtube.com/watch?v=8nkOV8UKpNM&list=PLxJ3eugu174Jm7Zj1Gx6Ex8-nGbPHFCEh&index=14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decodificador\n",
    "\n",
    "El decodificador traduce la frase una vez que conoce el significado.\n",
    "\n",
    "3 pasos:\n",
    "- **Auto atención** (self-attention)\n",
    "- **Atención cruzada** (encoder.decoder attention): Le pasamos los vectores de contexto + el resultado del codificador. La query la calculamos con los vectores de contexto y la matriz k y matriz v la calculamos con el resultado del codificador. De esta manera calculamos la atención cruzada entre el codificador y el descodificador. Esto nos devuelve otros vectores de contexto que los pasamos hacia el mecanísmo de propagación hacia adelante.\n",
    "- **Propagación hacia adelante** (feed forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos\n",
    "\n",
    "Ayudan a combinar la información nueva con la original y a mantener todo en un rango uniforme para que que el modelo funcione mejor en las próximas etapas.\n",
    "\n",
    "Frase original: \"El gato negro\"\n",
    "\n",
    "Los Transformers primero pasan esta frase por varias capas, incluyendo la capa de autoatención.\n",
    "\n",
    "**Paso 1: Vectorización**\n",
    "\n",
    "- \"El\": [1, 1, 1]\n",
    "- \"gato\": [1, 2, 3]\n",
    "- \"negro\": [2, 3, 4]\n",
    "\n",
    "**Paso 2: Linear (Lineal)**\n",
    "\n",
    "Aplicamos una transformación lineal a estos vectores. Digamos que tenemos una matriz de pesos W que multiplica estos vectores.\n",
    "\n",
    "W:\n",
    "(1 0 1\n",
    " 0 1 0\n",
    " 1 0 1)\n",
    " \n",
    "Multiplicamos cada vector por W:\n",
    "\n",
    "- \"El\": W * [0.3, 0.2, 0.2] = [0.5, 0.2, 0.5]\n",
    "- \"gato\": W * [0.3, 0.5, 0.7] = [1.0, 0.5, 1.0]\n",
    "- \"negro\": W * [0.5, 0.7, 0.9] = [1.4, 0.7, 1.4]\n",
    "\n",
    "**Paso 3: Autoatención**\n",
    "\n",
    "La autoatención procesa cada palabra y decide qué tanto debe prestar atención a las otras palabras en la frase. Por ejemplo, podría decidir que \"gato\" debería prestar mucha atención a \"negro\" porque es su descripción.\n",
    "\n",
    "**Paso 4: Softmax**\n",
    "\n",
    "Ahora aplicamos Softmax a los resultados para convertirlos en probabilidades.\n",
    "\n",
    "Calculamos exponentes:\n",
    "\n",
    "- \"El\": [exp(0.5), exp(0.2), exp(0.5)]\n",
    "- \"gato\": [exp(1.0), exp(0.5), exp(1.0)]\n",
    "- \"negro\": [exp(1.4), exp(0.7), exp(1.4)]\n",
    "\n",
    "Sumamos los exponentes:\n",
    "\n",
    "- \"El\": sum(exp(0.5), exp(0.2), exp(0.5))\n",
    "- \"gato\": sum(exp(1.0), exp(0.5), exp(1.0))\n",
    "- \"negro\": sum(exp(1.4), exp(0.7), exp(1.4))\n",
    "\n",
    "Calculamos las probabilidades:\n",
    "\n",
    "- \"El\": [exp(0.5)/suma, exp(0.2)/suma, exp(0.5)/suma]\n",
    "- \"gato\": [exp(1.0)/suma, exp(0.5)/suma, exp(1.0)/suma]\n",
    "- \"negro\": [exp(1.4)/suma, exp(0.7)/suma, exp(1.4)/suma]\n",
    "\n",
    "**Paso 5: Add (Suma)**\n",
    "\n",
    "Después de la autoatención, obtenemos nuevos vectores que representan las palabras con su nuevo contexto. Pero no queremos perder la información original de las palabras. Así que sumamos (Add) el vector original de cada palabra al nuevo vector obtenido después de la autoatención.\n",
    "\n",
    "- Vector original \"gato\": [1, 2, 3]\n",
    "- Vector después de la autoatención \"gato\": [2, 3, 4]\n",
    "- Vector combinado \"gato\": [1+2, 2+3, 3+4] = [3, 5, 7]\n",
    "\n",
    "**Paso 6: Norm (Normalización)**\n",
    "\n",
    "Después de sumar, ajustamos (normalizamos) los valores del vector combinado para que sean más uniformes y estén en un rango manejable. Esto ayuda a que los cálculos en las siguientes capas del Transformer sean más estables y eficientes.\n",
    "\n",
    "- Vector combinado \"gato\" (antes de la normalización): [3, 5, 7]\n",
    "- Vector normalizado \"gato\": [0.3, 0.5, 0.7] (estos son ejemplos simplificados)\n",
    "\n",
    "\n",
    "**Paso 7:Feed-Forward Network**\n",
    "\n",
    "Pasamos el vector normalizado por una red.\n",
    "\n",
    "El resultado puede ser algo como [2.8, 4.6, 6.1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 09:03:06.868727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tarea de clasificación\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.8972371816635132}]\n"
     ]
    }
   ],
   "source": [
    "res = classifier('Me encantan las clases de Nechu, explica genial')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.7578513622283936}]\n"
     ]
    }
   ],
   "source": [
    "res = classifier('El profesor es bueno, todo será sencillo')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta última frase realmente no es negativa, entonces ¿por qué la está puntuando así? Pues porque aquí:\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "No estamos especificando el modelo que queremos usar. El que viene por defento es: 'distilbert-base-uncased-finetuned-sst-2-english'. Y las frases que le hemos pasado no son en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997887015342712}]\n"
     ]
    }
   ],
   "source": [
    "res = classifier('I love Nechus classes, he explains great.')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998722076416016}]\n"
     ]
    }
   ],
   "source": [
    "res = classifier('The teacher is good, everything will be easy')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una amplia variedad de 'pipelines': https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo y tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo lo hemos ejecutado con apenas una línea, pero realmente hay bastantes etapas que ocurren por debajo. En el siguiente código vamos a ver las más importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero veamos cuales son las etapas anteriores con el mismo modelo\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.6551780104637146}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "res = classifier(\"El profesor es bueno todo será sencillo\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Para qué sirve el tokenizer?\n",
    "\n",
    "Codificación (antes del LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3449, 11268, 2229, 2953, 9686, 20934, 16515, 28681, 2080, 26358, 12411, 6895, 7174, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "secuencia = \"El profesor es bueno todo será sencillo\"\n",
    "res = tokenizer(secuencia)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['el', 'prof', '##es', '##or', 'es', 'bu', '##eno', 'tod', '##o', 'sera', 'sen', '##ci', '##llo']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(secuencia) #divide la frase en subdivisiones y se pierde el significado de la clase porque estamos usando un modelo en inglés.\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3449, 11268, 2229, 2953, 9686, 20934, 16515, 28681, 2080, 26358, 12411, 6895, 7174]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificar (después del LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] el profesor es bueno todo sera sencillo [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(res['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pysentimiento/robertuito-sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lunaflorestorres/.pyenv/versions/3.11.6/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POS', 'score': 0.9181905388832092}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "res = classifier(\"El profesor es bueno todo será sencillo\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 459, 5934, 442, 1220, 658, 1504, 9764, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Codificación del tokenizador\n",
    "\n",
    "secuencia = \"El profesor es bueno todo será sencillo\"\n",
    "res = tokenizer(secuencia)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁el', '▁profesor', '▁es', '▁bueno', '▁todo', '▁será', '▁sencillo']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(secuencia)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[459, 5934, 442, 1220, 658, 1504, 9764]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens) #id de cada una de las palabras que luego se convertirán en sus embeding\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> el profesor es bueno todo será sencillo</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decodificador\n",
    "\n",
    "tokenizer.decode(res['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar modelo y tokenizer en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = (\"./modelo\")\n",
    "tokenizer.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_local = AutoTokenizer.from_pretrained(model_path)\n",
    "model_local = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# También es compatible con Tensorflow (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"He estado deseando un curso así toda mi vida\",\n",
    "    \"Me encanta HuggingFace\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   723,  1524, 12667,   471,  4095,   816,  1001,   507,   837,\n",
      "             2],\n",
      "        [    0,   474,  2479,   925, 24020, 23912,     2,     1,     1,     1,\n",
      "             1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer( # toma las frases y las convierte en números\n",
    "    sentences,\n",
    "    padding=True, #  todas las frases tengan el mismo largo\n",
    "    truncation=True, # no sean demasiado largas\n",
    "    max_length=512, # no más de 512 palabras\n",
    "    return_tensors=\"pt\" # queremos los números en un formato que PyTorch\n",
    ")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # no necesita recordar lo que está haciendo ahora, solo queremos el resultado\n",
    "    outputs = model(**batch) # Aquí es donde el modelo toma las frases convertidas en números y nos da unos resultados llamados logits.\n",
    "    predictions = F.softmax(outputs.logits, dim=1) #  Los logits son números que el modelo produce, pero necesitamos convertirlos en probabilidades que sumen 1 (como porcentajes). softmax hace esto por nosotros.\n",
    "    labels = torch.argmax(predictions, dim=1) # cuál de las probabilidades es la más alta para cada frase. Esto nos dice si la frase es negativa, neutral o positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.6811, -0.0606,  0.9667],\n",
      "        [-1.8555, -0.3444,  2.4575]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Logits\n",
    "print(outputs) # Son los resultados crudos del modelo antes de convertirlos en probabilidades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1241, 0.2309, 0.6450],\n",
      "        [0.0125, 0.0565, 0.9310]])\n"
     ]
    }
   ],
   "source": [
    "# Neg, Neu, Pos\n",
    "print(predictions) # Esto nos dice la probabilidad de que cada frase sea negativa, neutral o positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# etiquetas\n",
    "print(labels) # Esto es la categoría final para cada frase (0 para negativa, 1 para neutral, 2 para positiva)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
